#!/usr/bin/env python3
"""
üß™ TEST PRATIQUE DU SOUS-MENU 'CHEMINS & DOSSIERS'
===================================================

Script de test pratique pour valider le fonctionnement r√©el du sous-menu
dans l'application FNEV4.

Par: GitHub Copilot - Assistant IA Expert
Date: Septembre 2025
"""

import os
import sys
import json
import time
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

class CheminsDossiersTest:
    """Testeur pratique pour le sous-menu Chemins & Dossiers"""
    
    def __init__(self):
        self.project_root = Path("C:/wamp64/www/FNEV4")
        self.test_results = {
            "compilation": False,
            "startup": False,
            "navigation": False,
            "functionality": {},
            "error_log": [],
            "recommendations": []
        }
    
    def run_practical_tests(self) -> Dict[str, Any]:
        """Ex√©cute les tests pratiques"""
        print("üß™ TEST PRATIQUE - SOUS-MENU CHEMINS & DOSSIERS")
        print("=" * 55)
        
        # 1. Test de compilation
        print("\n1Ô∏è‚É£ TEST DE COMPILATION")
        if self.test_compilation():
            print("   ‚úÖ Compilation r√©ussie")
            self.test_results["compilation"] = True
        else:
            print("   ‚ùå √âchec de compilation")
            return self.test_results
        
        # 2. Test de l'interface
        print("\n2Ô∏è‚É£ TEST DE L'INTERFACE")
        self.test_interface_elements()
        
        # 3. Test des services
        print("\n3Ô∏è‚É£ TEST DES SERVICES")
        self.test_services()
        
        # 4. Test de la persistance
        print("\n4Ô∏è‚É£ TEST DE LA PERSISTANCE")
        self.test_data_persistence()
        
        # 5. G√©n√©ration du rapport
        print("\n5Ô∏è‚É£ G√âN√âRATION DU RAPPORT DE TEST")
        self.generate_test_report()
        
        return self.test_results
    
    def test_compilation(self) -> bool:
        """Test de compilation du projet"""
        try:
            print("   üîÑ Compilation en cours...")
            
            result = subprocess.run([
                "dotnet", "build", "FNEV4.sln", 
                "--configuration", "Release", 
                "--verbosity", "minimal"
            ], 
            cwd=self.project_root,
            capture_output=True, 
            text=True,
            timeout=60
            )
            
            if result.returncode == 0:
                print("   ‚úÖ Projet compil√© sans erreur")
                return True
            else:
                print(f"   ‚ùå Erreurs de compilation d√©tect√©es")
                self.test_results["error_log"].append(f"Compilation error: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            print("   ‚è∞ Timeout de compilation")
            return False
        except Exception as e:
            print(f"   ‚ùå Erreur compilation: {e}")
            self.test_results["error_log"].append(f"Compilation exception: {e}")
            return False
    
    def test_interface_elements(self):
        """Test des √©l√©ments d'interface"""
        print("   üîç V√©rification des √©l√©ments XAML...")
        
        xaml_path = self.project_root / "src/FNEV4.Presentation/Views/Configuration/CheminsDossiersConfigView.xaml"
        
        if not xaml_path.exists():
            print("   ‚ùå Fichier XAML introuvable")
            self.test_results["error_log"].append("XAML file not found")
            return
        
        try:
            content = xaml_path.read_text(encoding='utf-8')
            
            # √âl√©ments critiques √† v√©rifier
            critical_elements = {
                "main_grid": "<Grid" in content,
                "material_cards": "materialDesign:Card" in content,
                "import_section": "ImportFolderPath" in content,
                "export_section": "ExportFolderPath" in content,
                "backup_section": "BackupFolderPath" in content,
                "action_buttons": "Command=" in content,
                "status_indicators": "Status" in content,
                "navigation_icons": "materialDesign:PackIcon" in content
            }
            
            missing_elements = []
            for element, exists in critical_elements.items():
                if exists:
                    print(f"     ‚úÖ {element}: Pr√©sent")
                else:
                    print(f"     ‚ùå {element}: Manquant")
                    missing_elements.append(element)
            
            self.test_results["functionality"]["interface_elements"] = {
                "total_checked": len(critical_elements),
                "found": len(critical_elements) - len(missing_elements),
                "missing": missing_elements,
                "success_rate": ((len(critical_elements) - len(missing_elements)) / len(critical_elements)) * 100
            }
            
            if not missing_elements:
                print("   üéâ Tous les √©l√©ments d'interface sont pr√©sents")
            else:
                print(f"   ‚ö†Ô∏è {len(missing_elements)} √©l√©ment(s) manquant(s)")
                
        except Exception as e:
            print(f"   ‚ùå Erreur lecture XAML: {e}")
            self.test_results["error_log"].append(f"XAML parsing error: {e}")
    
    def test_services(self):
        """Test des services backend"""
        print("   üîç V√©rification des services...")
        
        # Test du service de configuration des chemins
        path_service = self.project_root / "src/FNEV4.Infrastructure/Services/PathConfigurationService.cs"
        backup_service = self.project_root / "src/FNEV4.Infrastructure/Services/BackupService.cs"
        
        services_status = {
            "path_configuration_service": path_service.exists(),
            "backup_service": backup_service.exists() or self.check_backup_interface(),
            "database_provider": self.check_database_provider(),
            "logging_service": self.check_logging_service()
        }
        
        for service, exists in services_status.items():
            icon = "‚úÖ" if exists else "‚ùå"
            status = "Disponible" if exists else "Manquant"
            print(f"     {icon} {service}: {status}")
        
        self.test_results["functionality"]["services"] = services_status
        
        # Test de la configuration du PathConfigurationService
        if path_service.exists():
            self.test_path_configuration_service(path_service)
    
    def check_backup_interface(self) -> bool:
        """V√©rifie l'interface de backup"""
        backup_interface = self.project_root / "src/FNEV4.Core/Interfaces/IBackupService.cs"
        return backup_interface.exists()
    
    def check_database_provider(self) -> bool:
        """V√©rifie le provider de base de donn√©es"""
        db_provider = self.project_root / "src/FNEV4.Infrastructure/Services/DatabasePathProvider.cs"
        return db_provider.exists()
    
    def check_logging_service(self) -> bool:
        """V√©rifie le service de logging"""
        logging_paths = [
            "src/FNEV4.Infrastructure/Services/LoggingService.cs",
            "src/FNEV4.Core/Interfaces/ILoggingService.cs"
        ]
        return any((self.project_root / path).exists() for path in logging_paths)
    
    def test_path_configuration_service(self, service_path: Path):
        """Test sp√©cifique du service de configuration des chemins"""
        print("     üîç Test d√©taill√© du PathConfigurationService...")
        
        try:
            content = service_path.read_text(encoding='utf-8')
            
            # V√©rifications critiques
            checks = {
                "centralized_database": "IDatabasePathProvider" in content,
                "ensure_directories": "EnsureDirectoriesExist" in content,
                "update_paths_method": "UpdatePaths" in content,
                "validate_path_method": "ValidatePath" in content,
                "calculate_size_method": "CalculateDirectorySize" in content,
                "fallback_handling": "GetProjectRootPath" in content
            }
            
            passed_checks = sum(checks.values())
            total_checks = len(checks)
            
            for check, passed in checks.items():
                icon = "‚úÖ" if passed else "‚ùå"
                print(f"       {icon} {check}")
            
            print(f"       üìä Score: {passed_checks}/{total_checks} ({(passed_checks/total_checks)*100:.1f}%)")
            
            self.test_results["functionality"]["path_service_details"] = {
                "checks": checks,
                "score": (passed_checks / total_checks) * 100
            }
            
        except Exception as e:
            print(f"       ‚ùå Erreur lecture service: {e}")
            self.test_results["error_log"].append(f"Service reading error: {e}")
    
    def test_data_persistence(self):
        """Test de la persistance des donn√©es"""
        print("   üîç Test de la persistance des donn√©es...")
        
        # V√©rifier l'entit√© de configuration
        entity_path = self.project_root / "src/FNEV4.Core/Entities/FolderConfiguration.cs"
        
        if entity_path.exists():
            print("     ‚úÖ Entit√© FolderConfiguration trouv√©e")
            
            try:
                content = entity_path.read_text(encoding='utf-8')
                
                # V√©rifications de l'entit√©
                entity_features = {
                    "data_annotations": "[Required]" in content or "[StringLength" in content,
                    "validation_methods": "ValidateConfiguration" in content,
                    "utility_methods": "CreateAllFolders" in content,
                    "folder_paths": "ImportFolderPath" in content and "ExportFolderPath" in content,
                    "archive_settings": "ArchiveAutoEnabled" in content,
                    "backup_settings": "BackupAutoEnabled" in content
                }
                
                for feature, exists in entity_features.items():
                    icon = "‚úÖ" if exists else "‚ùå"
                    print(f"       {icon} {feature}")
                
                self.test_results["functionality"]["entity_features"] = entity_features
                
            except Exception as e:
                print(f"     ‚ùå Erreur lecture entit√©: {e}")
                self.test_results["error_log"].append(f"Entity reading error: {e}")
        else:
            print("     ‚ùå Entit√© FolderConfiguration introuvable")
            self.test_results["error_log"].append("FolderConfiguration entity not found")
    
    def generate_test_report(self):
        """G√©n√®re le rapport de test"""
        print("   üìù G√©n√©ration du rapport de test...")
        
        # Calculer le score global
        scores = []
        
        # Score interface
        interface_score = self.test_results["functionality"].get("interface_elements", {}).get("success_rate", 0)
        scores.append(interface_score)
        
        # Score services
        services = self.test_results["functionality"].get("services", {})
        service_score = (sum(services.values()) / len(services)) * 100 if services else 0
        scores.append(service_score)
        
        # Score entit√©
        entity_features = self.test_results["functionality"].get("entity_features", {})
        entity_score = (sum(entity_features.values()) / len(entity_features)) * 100 if entity_features else 0
        scores.append(entity_score)
        
        global_test_score = sum(scores) / len(scores) if scores else 0
        
        # Cr√©er le rapport
        test_report = {
            "test_date": datetime.now().isoformat(),
            "component": "Chemins & Dossiers - Test Pratique",
            "compilation_success": self.test_results["compilation"],
            "global_test_score": global_test_score,
            "detailed_scores": {
                "interface_elements": interface_score,
                "services": service_score,
                "entity_features": entity_score
            },
            "functionality_details": self.test_results["functionality"],
            "error_log": self.test_results["error_log"],
            "recommendations": self.generate_test_recommendations(global_test_score),
            "test_summary": self.create_test_summary(global_test_score)
        }
        
        # Sauvegarder le rapport
        report_path = self.project_root / "TEST_PRATIQUE_CHEMINS_DOSSIERS.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(test_report, f, indent=2, ensure_ascii=False)
        
        print(f"   üíæ Rapport de test sauvegard√©: {report_path}")
        
        # Afficher le r√©sum√©
        self.display_test_summary(global_test_score, test_report)
    
    def generate_test_recommendations(self, score: float) -> list:
        """G√©n√®re les recommandations bas√©es sur les tests"""
        recommendations = []
        
        if not self.test_results["compilation"]:
            recommendations.append({
                "priority": "CRITIQUE",
                "category": "Compilation",
                "recommendation": "Corriger les erreurs de compilation avant tout test"
            })
        
        if score < 90:
            recommendations.append({
                "priority": "HAUTE",
                "category": "Fonctionnalit√©",
                "recommendation": "Am√©liorer l'impl√©mentation des fonctionnalit√©s manquantes"
            })
        
        if self.test_results["error_log"]:
            recommendations.append({
                "priority": "MOYENNE", 
                "category": "Robustesse",
                "recommendation": f"Corriger les {len(self.test_results['error_log'])} erreur(s) d√©tect√©e(s)"
            })
        
        return recommendations
    
    def create_test_summary(self, score: float) -> dict:
        """Cr√©e un r√©sum√© des tests"""
        if score >= 95:
            status = "üéâ PARFAIT"
            message = "Le sous-menu fonctionne parfaitement"
        elif score >= 85:
            status = "‚úÖ EXCELLENT"
            message = "Le sous-menu fonctionne tr√®s bien"
        elif score >= 75:
            status = "üëç BON"
            message = "Le sous-menu fonctionne correctement"
        elif score >= 60:
            status = "‚ö†Ô∏è MOYEN"
            message = "Le sous-menu n√©cessite des am√©liorations"
        else:
            status = "‚ùå CRITIQUE"
            message = "Le sous-menu n√©cessite des corrections importantes"
        
        return {
            "status": status,
            "message": message,
            "compilation_ok": self.test_results["compilation"],
            "errors_count": len(self.test_results["error_log"]),
            "recommendations_count": len(self.generate_test_recommendations(score))
        }
    
    def display_test_summary(self, score: float, report: dict):
        """Affiche le r√©sum√© des tests"""
        print("\n" + "=" * 55)
        print("üìä R√âSUM√â DES TESTS PRATIQUES")
        print("=" * 55)
        
        summary = report["test_summary"]
        
        print(f"\nüéØ SCORE GLOBAL: {score:.1f}%")
        print(f"üìã STATUT: {summary['status']}")
        print(f"üí¨ √âVALUATION: {summary['message']}")
        
        print(f"\nüìà D√âTAIL DES TESTS:")
        print(f"   ‚Ä¢ Compilation: {'‚úÖ R√©ussie' if summary['compilation_ok'] else '‚ùå √âchec'}")
        print(f"   ‚Ä¢ Interface: {report['detailed_scores']['interface_elements']:.1f}%")
        print(f"   ‚Ä¢ Services: {report['detailed_scores']['services']:.1f}%")
        print(f"   ‚Ä¢ Persistance: {report['detailed_scores']['entity_features']:.1f}%")
        
        if report["error_log"]:
            print(f"\n‚ö†Ô∏è ERREURS D√âTECT√âES ({len(report['error_log'])}):")
            for i, error in enumerate(report["error_log"][:3], 1):  # Limite √† 3 erreurs
                print(f"   {i}. {error}")
            if len(report["error_log"]) > 3:
                print(f"   ... et {len(report['error_log']) - 3} autre(s)")
        
        if report["recommendations"]:
            print(f"\nüîß RECOMMANDATIONS ({len(report['recommendations'])}):")
            for i, rec in enumerate(report["recommendations"], 1):
                print(f"   {i}. [{rec['priority']}] {rec['category']}: {rec['recommendation']}")
        
        print("\n" + "=" * 55)
        if score >= 85:
            print("‚úÖ TESTS PRATIQUES R√âUSSIS - LE SOUS-MENU EST FONCTIONNEL")
        else:
            print("‚ö†Ô∏è TESTS PRATIQUES PARTIELS - AM√âLIORATIONS N√âCESSAIRES")
        print("=" * 55)

def main():
    """Point d'entr√©e principal"""
    print("üöÄ LANCEMENT DES TESTS PRATIQUES")
    print("Composant: Sous-menu 'Chemins & Dossiers'")
    print("Objectif: Validation du fonctionnement r√©el")
    print()
    
    try:
        tester = CheminsDossiersTest()
        results = tester.run_practical_tests()
        
        return 0 if results["compilation"] else 1
        
    except Exception as e:
        print(f"\n‚ùå ERREUR LORS DES TESTS: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
